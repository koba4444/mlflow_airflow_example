{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyyoutube'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1b32b63a7649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyyoutube\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyyoutube'"
     ]
    }
   ],
   "source": [
    "from pyyoutube import Api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "from pyyoutube import Api\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import f1_score, silhouette_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(YOUTUBE_API_KEY, videoId, maxResults, nextPageToken):\n",
    "    \"\"\"\n",
    "    Получение информации со страницы с видео\n",
    "    \"\"\"\n",
    "    YOUTUBE_URI = 'https://www.googleapis.com/youtube/v3/commentThreads?key={KEY}&textFormat=plainText&' + \\\n",
    "        'part=snippet&videoId={videoId}&maxResults={maxResults}&pageToken={nextPageToken}'\n",
    "    format_youtube_uri = YOUTUBE_URI.format(KEY=YOUTUBE_API_KEY,\n",
    "                                            videoId=videoId,\n",
    "                                            maxResults=maxResults,\n",
    "                                            nextPageToken=nextPageToken)\n",
    "    content = requests.get(format_youtube_uri).text\n",
    "    data = json.loads(content)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_text_of_comment(data):\n",
    "    \"\"\"\n",
    "    Получение комментариев из полученных данных под одним видео\n",
    "    \"\"\"\n",
    "    comms = set()\n",
    "    for item in data['items']:\n",
    "        comm = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        comms.add(comm)\n",
    "    return comms\n",
    "\n",
    "\n",
    "def get_all_comments(YOUTUBE_API_KEY, query, count_video=10, limit=30, maxResults=10, nextPageToken=''):\n",
    "    \"\"\"\n",
    "    Выгрузка maxResults комментариев\n",
    "    \"\"\"\n",
    "    api = Api(api_key=YOUTUBE_API_KEY)\n",
    "    video_by_keywords = api.search_by_keywords(q=query,\n",
    "                                               search_type=[\"video\"],\n",
    "                                               count=count_video,\n",
    "                                               limit=limit)\n",
    "    videoId = [x.id.videoId for x in video_by_keywords.items]\n",
    "\n",
    "    comments_all = []\n",
    "    for id_video in videoId:\n",
    "        try:\n",
    "            data = get_data(YOUTUBE_API_KEY,\n",
    "                            id_video,\n",
    "                            maxResults=maxResults,\n",
    "                            nextPageToken=nextPageToken)\n",
    "            comment = list(get_text_of_comment(data))\n",
    "            comments_all.append(comment)\n",
    "        except:\n",
    "            continue\n",
    "    comments = sum(comments_all, [])\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join('../config/params_all.yaml')\n",
    "config = yaml.safe_load(open(config_path))['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = config['SEED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = get_all_comments(**config['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    \"\"\"\n",
    "    Удаление эмоджи из текста\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "def remove_links(string):\n",
    "    \"\"\"\n",
    "    Удаление ссылок\n",
    "    \"\"\"\n",
    "    string = re.sub(r'http\\S+', '', string)  # remove http links\n",
    "    string = re.sub(r'bit.ly/\\S+', '', string)  # rempve bitly links\n",
    "    string = re.sub(r'www\\S+', '', string)  # rempve bitly links\n",
    "    string = string.strip('[link]')  # remove [links]\n",
    "    return string\n",
    "\n",
    "\n",
    "def preprocessing(string, stopwords, stem):\n",
    "    \"\"\"\n",
    "    Простой препроцессинг текста, очистка, лематизация, удаление коротких слов\n",
    "    \"\"\"\n",
    "    string = remove_emoji(string)\n",
    "    string = remove_links(string)\n",
    "\n",
    "    # удаление символов \"\\r\\n\"\n",
    "    str_pattern = re.compile(\"\\r\\n\")\n",
    "    string = str_pattern.sub(r'', string)\n",
    "\n",
    "    # очистка текста от символов\n",
    "    string = re.sub('(((?![а-яА-Я ]).)+)', ' ', string)\n",
    "    # лематизация\n",
    "    string = ' '.join([\n",
    "        re.sub('\\\\n', '', ' '.join(stem.lemmatize(s))).strip()\n",
    "        for s in string.split()\n",
    "    ])\n",
    "    # удаляем слова короче 3 символов\n",
    "    string = ' '.join([s for s in string.split() if len(s) > 3])\n",
    "    # удаляем стоп-слова\n",
    "    string = ' '.join([s for s in string.split() if s not in stopwords])\n",
    "    return string\n",
    "\n",
    "\n",
    "def get_clean_text(data, stopwords):\n",
    "    \"\"\"\n",
    "    Получение текста в преобразованной после очистки\n",
    "    матричном виде, а также модель векторизации\n",
    "    \"\"\"\n",
    "    # Простой препроцессинг текста\n",
    "    stem = Mystem()\n",
    "    comments = [preprocessing(x, stopwords, stem) for x in data]\n",
    "    # Удаление комментов, которые имеют меньше, чем 5 слов\n",
    "    comments = [y for y in comments if len(y.split()) > 5]\n",
    "    #common_texts = [i.split(' ') for i in comments]\n",
    "    return comments\n",
    "\n",
    "\n",
    "def vectorize_text(data, tfidf):\n",
    "    \"\"\"\n",
    "    Получение матрицы кол-ва слов в комменариях\n",
    "    Очистка от пустых строк\n",
    "    \"\"\"\n",
    "    # Векторизация\n",
    "    X_matrix = tfidf.transform(data).toarray()\n",
    "    # Удаляем строки в матрице с пустыми значениями\n",
    "    mask = (np.nan_to_num(X_matrix) != 0).any(axis=1)\n",
    "    return X_matrix[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean = get_clean_text(comments, stopwords.words(config['stopwords']))\n",
    "tfidf = TfidfVectorizer(**config['tf_model']).fit(comments_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix = vectorize_text(comments_clean, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(data, count_max_clusters, random_state, affinity,\n",
    "                 silhouette_metric):\n",
    "    \"\"\"\n",
    "    Подбор наилучшего числа кластеров, возвращает полученные кластера тематик\n",
    "    \"\"\"\n",
    "    cluster_labels = {}\n",
    "    silhouette_mean = []\n",
    "\n",
    "    for i in range(2, count_max_clusters, 1):\n",
    "        clf = SpectralClustering(n_clusters=i,\n",
    "                                 affinity=affinity,\n",
    "                                 random_state=random_state)\n",
    "        #clf = KMeans(n_clusters=n, max_iter=1000, n_init=1)\n",
    "        clf.fit(data)\n",
    "        labels = clf.labels_\n",
    "        cluster_labels[i] = labels\n",
    "        silhouette_mean.append(\n",
    "            silhouette_score(data, labels, metric=silhouette_metric))\n",
    "    n_clusters = silhouette_mean.index(max(silhouette_mean)) + 2\n",
    "    return cluster_labels[n_clusters]\n",
    "\n",
    "\n",
    "def get_f1_score(y_test, y_pred, unique_cluster_labels):\n",
    "    \"\"\"\n",
    "    Возращает результат обучения классификатора по тематикам\n",
    "    \"\"\"\n",
    "    return f1_score(\n",
    "        y_test, y_pred,\n",
    "        average='macro') \\\n",
    "        if len(unique_cluster_labels) > 2 \\\n",
    "        else f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = get_clusters(X_matrix,\n",
    "                                 random_state=SEED,\n",
    "                                 **config['clustering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_matrix,\n",
    "                                                    cluster_labels,\n",
    "                                                    **config['cross_val'],\n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(**config['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MLFLOW_REGISTRY_URI=../mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(config['name_experiment'])\n",
    "with mlflow.start_run():\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    print(clf_lr.predict_proba(X_test))\n",
    "\n",
    "    # Логирование модели и параметров\n",
    "    mlflow.log_param(\n",
    "        'f1', get_f1_score(y_test, clf_lr.predict(X_test),\n",
    "                           set(cluster_labels)))\n",
    "    mlflow.sklearn.log_model(\n",
    "        tfidf,\n",
    "        artifact_path=\"vector\",\n",
    "        registered_model_name=f\"{config['model_vec']}\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        clf_lr,\n",
    "        artifact_path='model_lr',\n",
    "        registered_model_name=f\"{config['model_lr']}\")\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.get_artifact_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_version_model(config_name, client):\n",
    "    \"\"\"\n",
    "    Получение последней версии модели из MLFlow\n",
    "    \"\"\"\n",
    "    dict_push = {}\n",
    "    for count, value in enumerate(\n",
    "        client.search_model_versions(f\"name='{config_name}'\")):\n",
    "        # client.list_registered_models()):\n",
    "        # Все версии модели\n",
    "        dict_push[count] = value\n",
    "    return dict(list(dict_push.items())[-1][1])['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "last_version_lr = get_version_model(config['model_lr'], client)\n",
    "last_version_vec = get_version_model(config['model_vec'], client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_version_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_version_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
